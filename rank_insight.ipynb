{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_envt():\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged perceptron tagger')\n",
    "    print \"Download complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tags(tagged, tags=['NN','JJ','NNP']):\n",
    "    filtered=[]\n",
    "    for item in tagged:\n",
    "        if item[1] in tags:\n",
    "            filtered.append(item)\n",
    "    print \"All tokens tagged.\"\n",
    "    return filtered\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(tagged):\n",
    "    norm=[]\n",
    "    for item in tagged:\n",
    "        norm.append(item[0].replace('.',''),item[1])\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_set(iterable, key=None):\n",
    "    seen=set()\n",
    "    seen_add=seen.add\n",
    "    if key is None:\n",
    "        for element in [x for x in iterable if x not in seen]:\n",
    "            seen_add(element)\n",
    "            yield element\n",
    "    else:\n",
    "        for element in iterable:\n",
    "            k = key(element)\n",
    "            if k not in seen:\n",
    "                seen_add(k)\n",
    "                yield element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(nodes):\n",
    "    gr=nx.Graph()\n",
    "    gr.add_nodes_from(nodes)\n",
    "    pairednodes=list(itertools.combinations(nodes,2))\n",
    "    for pair in pairednodes:\n",
    "        fstring=pair[0]\n",
    "        sstring=pair[1]\n",
    "        levDist=editdistance.eval(fstring,sstring)\n",
    "        gr.add_edge(fstring,sstring,weight=levDist)\n",
    "    return gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " #extracting keywords and key-phrases for a document queried.\n",
    "def extract_keyphrases(text):\n",
    "    word_tokens=nltk.word_tokenize(text)\n",
    "    tagged=nltk.pos_tag(word_tokens)\n",
    "    textlist=[x[0] for x in tagged]\n",
    "    \n",
    "    tagged=filter_tags(tagged)\n",
    "    tagged=normalize(tagged)\n",
    "    \n",
    "    unique_words=unique_set([x[0] for x in tagged])\n",
    "    word_set=list(unique_words)\n",
    "    \n",
    "    graph=build_graph(word_set)\n",
    "    calc_rank=nx.pagerank(graph,weight='weight')\n",
    "    \n",
    "    keyphrases=sorted(calc_rank, key=calc_rank.get, reverse=True)\n",
    "    keyphrases=keyphrases[0:len(wordset)//3 + 1]\n",
    "    modified_phrases=set([])\n",
    "    dealt_with=set([])\n",
    "    i,j=0,0\n",
    "    while j<len(textlist):\n",
    "        first=textlist[i]\n",
    "        second=textlist[j]\n",
    "        if first and second in keyphrases:\n",
    "            keyphrase = first + ' ' + second #create phrase\n",
    "            modified_phrases.add(keyphrase)\n",
    "            dealt_with.add(first)\n",
    "            dealt_with.add(second)\n",
    "        else:\n",
    "            if first in keyphrases and first not in dealt_with:\n",
    "                modified_phrases.add(first)\n",
    "            if j==len(textlist)-1 and second in keyphrases and second not in dealt_with:\n",
    "                modified_phrases.add(second)\n",
    "        i=i+1; j=j+1\n",
    "        \n",
    "    return modified_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(text, summary_len, clean_sentences=False, language='English'):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/'+language+'.pickle')\n",
    "    sentence_tokens = sent_detector.tokenize(text.strip())\n",
    "    graph = build_graph(sentence_tokens)\n",
    "\n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "    sentences = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
    "    summary = ' '.join(sentences)\n",
    "    summary_words = summary.split()\n",
    "    summary_words = summary_words[0:summary_length]\n",
    "    dot_indices = [idx for idx, word in enumerate(summary_words) if word.find('.') != -1]\n",
    "    if clean_sentences and dot_indices:\n",
    "        last_dot = max(dot_indices) + 1\n",
    "        summary = ' '.join(summary_words[0:last_dot])\n",
    "    else:\n",
    "        summary = ' '.join(summary_words)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output(summary, key_phrases, fname):\n",
    "    kp_file=open(fname,\"w+\")\n",
    "    for item in key_phrases:\n",
    "        kp_file.write(item+\"\\n\")\n",
    "    kp_file.close()\n",
    "    print \"KEYWORDS GENERATED\"\n",
    "    \n",
    "    sum_file=open(\"summary_\"+fname,\"w+\")\n",
    "    sum_file.write(summary)\n",
    "    sum_file.close()\n",
    "    print \"SUMMARY GENERATED\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef summarize():\\n    #sample check for NLP\\n    articles = os.listdir(\"articles\")\\n    for article in articles:\\n        print(\\'Reading articles/\\' + article)\\n        article_file = io.open(\\'articles/\\' + article, \\'r\\')\\n        text = article_file.read()\\n        keyphrases = extract_keyphrases(text)\\n        summary = extract(text)\\n        check_output(summary, keyphrases, article)\\n    \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def summarize():\n",
    "    #sample check for NLP\n",
    "    articles = os.listdir(\"articles\")\n",
    "    for article in articles:\n",
    "        print('Reading articles/' + article)\n",
    "        article_file = io.open('articles/' + article, 'r')\n",
    "        text = article_file.read()\n",
    "        keyphrases = extract_keyphrases(text)\n",
    "        summary = extract(text)\n",
    "        check_output(summary, keyphrases, article)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
